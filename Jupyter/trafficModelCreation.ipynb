{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4852d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1060 6GB\n",
      "AMP enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deyru\\AppData\\Local\\Temp\\ipykernel_16768\\1303336646.py:181: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "C:\\Users\\deyru\\AppData\\Local\\Temp\\ipykernel_16768\\1303336646.py:218: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "C:\\Users\\deyru\\AppData\\Local\\Temp\\ipykernel_16768\\1303336646.py:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/40 | train_loss: 1.2787 | val_loss: 1.1996 | val_acc: 0.3294\n",
      "Epoch 02/40 | train_loss: 1.1168 | val_loss: 1.2489 | val_acc: 0.4167\n",
      "Epoch 03/40 | train_loss: 0.9822 | val_loss: 1.1432 | val_acc: 0.4405\n",
      "Epoch 04/40 | train_loss: 0.8496 | val_loss: 0.9183 | val_acc: 0.5833\n",
      "Epoch 05/40 | train_loss: 0.7211 | val_loss: 1.0120 | val_acc: 0.5556\n",
      "Epoch 06/40 | train_loss: 0.5900 | val_loss: 1.6368 | val_acc: 0.4960\n",
      "Epoch 07/40 | train_loss: 0.4890 | val_loss: 1.8736 | val_acc: 0.4167\n",
      "[LR] reduced: 0.001 -> 0.0005\n",
      "Epoch 08/40 | train_loss: 0.3955 | val_loss: 1.8268 | val_acc: 0.4921\n",
      "Epoch 09/40 | train_loss: 0.2229 | val_loss: 0.6027 | val_acc: 0.7897\n",
      "Epoch 10/40 | train_loss: 0.0960 | val_loss: 0.2646 | val_acc: 0.9206\n",
      "Epoch 11/40 | train_loss: 0.0735 | val_loss: 0.5133 | val_acc: 0.8056\n",
      "Epoch 12/40 | train_loss: 0.0700 | val_loss: 0.4608 | val_acc: 0.8413\n",
      "Epoch 13/40 | train_loss: 0.0559 | val_loss: 0.2718 | val_acc: 0.9127\n",
      "[LR] reduced: 0.0005 -> 0.00025\n",
      "Epoch 14/40 | train_loss: 0.0473 | val_loss: 0.2860 | val_acc: 0.9127\n",
      "Epoch 15/40 | train_loss: 0.0295 | val_loss: 0.1861 | val_acc: 0.9444\n",
      "Epoch 16/40 | train_loss: 0.0183 | val_loss: 0.1911 | val_acc: 0.9444\n",
      "Epoch 17/40 | train_loss: 0.0116 | val_loss: 0.1589 | val_acc: 0.9563\n",
      "Epoch 18/40 | train_loss: 0.0086 | val_loss: 0.1664 | val_acc: 0.9484\n",
      "Epoch 19/40 | train_loss: 0.0098 | val_loss: 0.1621 | val_acc: 0.9524\n",
      "Epoch 20/40 | train_loss: 0.0082 | val_loss: 0.1690 | val_acc: 0.9563\n",
      "Epoch 21/40 | train_loss: 0.0061 | val_loss: 0.1424 | val_acc: 0.9643\n",
      "Epoch 22/40 | train_loss: 0.0040 | val_loss: 0.1482 | val_acc: 0.9643\n",
      "Epoch 23/40 | train_loss: 0.0045 | val_loss: 0.1490 | val_acc: 0.9603\n",
      "Epoch 24/40 | train_loss: 0.0055 | val_loss: 0.1551 | val_acc: 0.9603\n",
      "[LR] reduced: 0.00025 -> 0.000125\n",
      "Epoch 25/40 | train_loss: 0.0076 | val_loss: 0.1648 | val_acc: 0.9444\n",
      "Epoch 26/40 | train_loss: 0.0025 | val_loss: 0.1564 | val_acc: 0.9524\n",
      "Epoch 27/40 | train_loss: 0.0028 | val_loss: 0.1545 | val_acc: 0.9563\n",
      "Epoch 28/40 | train_loss: 0.0027 | val_loss: 0.1554 | val_acc: 0.9563\n",
      "Early stopping at epoch 28 (no val_acc improvement for 7 epochs).\n",
      "Best validation accuracy:  0.9643\n",
      "Final validation accuracy: 0.9563\n",
      "Saved state to: ../Models\\traffic_classifier_state.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deyru\\AppData\\Local\\Temp\\ipykernel_16768\\1303336646.py:147: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if H < 224 or W < 224:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript model to: ../Models\\traffic_classifier_scripted.pt\n"
     ]
    }
   ],
   "source": [
    "# traffic_classifier_resnet.py\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------\n",
    "# Reproducibility\n",
    "# --------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# --------------------\n",
    "# Paths (as requested)\n",
    "# --------------------\n",
    "IMAGES_PATH = \"../output/traffic_lights_images.npy\"\n",
    "LABELS_PATH = \"../output/traffic_lights_label.npy\"\n",
    "SAVE_DIR = \"../Models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------\n",
    "# cuDNN speedups (good for fixed input size)\n",
    "# --------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# --------------------\n",
    "# Load data (NumPy)\n",
    "# --------------------\n",
    "X = np.load(IMAGES_PATH)  # (N,H,W) or (N,H,W,C)\n",
    "y_raw = np.load(LABELS_PATH)  # (N,) with values in {-1,0,1}\n",
    "\n",
    "if X.ndim not in (3, 4):\n",
    "    raise ValueError(f\"Expected images with 3 or 4 dims (N,H,W[,C]); got {X.shape}\")\n",
    "\n",
    "# Ensure channel dimension and channel-first format\n",
    "if X.ndim == 3:\n",
    "    # (N,H,W) -> (N,1,H,W)\n",
    "    X = np.expand_dims(X, axis=1)\n",
    "else:\n",
    "    # (N,H,W,C) -> (N,C,H,W)\n",
    "    X = np.transpose(X, (0, 3, 1, 2))\n",
    "\n",
    "# Normalize to [0,1]\n",
    "X = X.astype(\"float32\")\n",
    "if X.max() > 1.5:\n",
    "    X /= 255.0\n",
    "\n",
    "# Remap labels {-1,0,1} -> {0,1,2}\n",
    "label_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "try:\n",
    "    y = np.vectorize(label_to_index.__getitem__)(y_raw)\n",
    "except KeyError as e:\n",
    "    raise ValueError(f\"Unknown label {e}; expected only -1, 0, 1\")\n",
    "\n",
    "num_classes = 3\n",
    "in_channels = X.shape[1]\n",
    "\n",
    "# --------------------\n",
    "# Convert to tensors once (fast path)\n",
    "# --------------------\n",
    "X_torch = torch.from_numpy(X).contiguous()                    # (N,C,H,W) float32 in [0,1]\n",
    "y_torch = torch.from_numpy(y.astype(np.int64)).contiguous()   # (N,)\n",
    "\n",
    "# --------------------\n",
    "# Split 80/20 (stratified)\n",
    "# --------------------\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
    "    X_torch.numpy(), y_torch.numpy(),\n",
    "    test_size=0.2, random_state=SEED, stratify=y_torch.numpy()\n",
    ")\n",
    "X_train = torch.from_numpy(X_train_np).contiguous()\n",
    "X_val   = torch.from_numpy(X_val_np).contiguous()\n",
    "y_train = torch.from_numpy(y_train_np).contiguous()\n",
    "y_val   = torch.from_numpy(y_val_np).contiguous()\n",
    "\n",
    "# --------------------\n",
    "# DataLoaders\n",
    "# NOTE: ResNet uses more memory than the small CNN. Start with smaller batches.\n",
    "# --------------------\n",
    "def _workers():\n",
    "    c = os.cpu_count() or 2\n",
    "    return max(min(4, c - 1), 0)\n",
    "\n",
    "num_workers = _workers()\n",
    "persistent = True if num_workers > 0 else False\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=64, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=persistent\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=128, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=persistent\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Model: ResNet-18 backbone adapted to 1 or 3 channels\n",
    "# --------------------\n",
    "def build_resnet_classifier(in_channels: int, num_classes: int):\n",
    "    # Handle torchvision version differences for weights/pretrained arg\n",
    "    try:\n",
    "        from torchvision.models import resnet18, ResNet18_Weights  # newer API\n",
    "        backbone = resnet18(weights=None)\n",
    "    except Exception:\n",
    "        from torchvision.models import resnet18  # fallback\n",
    "        backbone = resnet18(pretrained=False)\n",
    "\n",
    "    # Adapt first conv to input channels if needed\n",
    "    if in_channels != 3:\n",
    "        backbone.conv1 = nn.Conv2d(\n",
    "            in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "\n",
    "    # Replace the classification head\n",
    "    in_feat = backbone.fc.in_features\n",
    "    backbone.fc = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(in_feat, num_classes)\n",
    "    )\n",
    "\n",
    "    # Wrap in a module that ensures input is big enough (ResNet expects ~224x224)\n",
    "    class ResNetClassifier(nn.Module):\n",
    "        def __init__(self, net):\n",
    "            super().__init__()\n",
    "            self.net = net\n",
    "\n",
    "        def forward(self, x):\n",
    "            # If H or W is small, upscale to 224 (no-op if already >=224)\n",
    "            if x.dim() == 4:\n",
    "                _, _, H, W = x.shape\n",
    "                if H < 224 or W < 224:\n",
    "                    x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "            return self.net(x)\n",
    "\n",
    "    return ResNetClassifier(backbone)\n",
    "\n",
    "# --------------------\n",
    "# Device & AMP\n",
    "# --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"AMP enabled:\", torch.cuda.is_available())\n",
    "\n",
    "model = build_resnet_classifier(in_channels=in_channels, num_classes=num_classes).to(device)\n",
    "\n",
    "# --------------------\n",
    "# Loss / Optim / Scheduler\n",
    "# --------------------\n",
    "# Optional: class weights if imbalanced\n",
    "ctr = Counter(y_train.tolist())\n",
    "counts = np.array([ctr.get(i, 0) for i in range(num_classes)], dtype=np.float32)\n",
    "if np.any(counts == 0):\n",
    "    class_weights = None\n",
    "else:\n",
    "    inv = 1.0 / counts\n",
    "    class_weights = torch.tensor(inv / inv.sum() * num_classes, dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3, min_lr=1e-6\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# --------------------\n",
    "# Training / Eval loops\n",
    "# --------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total, correct, run_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            run_loss += loss.item() * yb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return run_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "EPOCHS = 40\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "patience = 7\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    run_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        run_loss += loss.item() * yb.size(0)\n",
    "        seen += yb.size(0)\n",
    "\n",
    "    train_loss = run_loss / max(seen, 1)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != old_lr:\n",
    "        print(f\"[LR] reduced: {old_lr:.6g} -> {new_lr:.6g}\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_acc\": float(val_acc),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"in_channels\": int(in_channels),\n",
    "            \"num_classes\": int(num_classes),\n",
    "        }\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val_acc improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# Load best and evaluate once more\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model_state_dict\"])\n",
    "final_val_loss, final_val_acc = evaluate(val_loader)\n",
    "print(f\"Best validation accuracy:  {best_val_acc:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# --------------------\n",
    "# Save artifacts\n",
    "# --------------------\n",
    "# 1) state_dict bundle (for finetuning/resume)\n",
    "pth_path = os.path.join(SAVE_DIR, \"traffic_classifier_state.pth\")\n",
    "torch.save(best_state if best_state is not None else model.state_dict(), pth_path)\n",
    "print(f\"Saved state to: {pth_path}\")\n",
    "\n",
    "# 2) TorchScript (for deployment/inference)\n",
    "example = X_val[:1].to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scripted = torch.jit.trace(model, example)\n",
    "ts_path = os.path.join(SAVE_DIR, \"traffic_classifier_scripted.pt\")\n",
    "scripted.save(ts_path)\n",
    "print(f\"Saved TorchScript model to: {ts_path}\")\n",
    "\n",
    "# 3) class mapping & metrics\n",
    "with open(os.path.join(SAVE_DIR, \"class_mapping.json\"), \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"index_to_label\": {str(k): int(v) for k, v in index_to_label.items()},\n",
    "            \"label_to_index\": {str(k): int(v) for k, v in label_to_index.items()},\n",
    "            \"semantics\": {\"-1\": \"no light\", \"0\": \"red\", \"1\": \"green\"}\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"metrics.txt\"), \"w\") as f:\n",
    "    f.write(f\"Final validation accuracy: {final_val_acc:.4f}\\n\")\n",
    "    f.write(f\"Final validation loss: {final_val_loss:.4f}\\n\")\n",
    "    f.write(f\"Best validation accuracy: {best_val_acc:.4f}\\n\")\n",
    "\n",
    "# --------------------\n",
    "# Quick prediction helper (numpy in, original labels out)\n",
    "# --------------------\n",
    "def predict_labels_numpy(np_batch: np.ndarray, batch_size: int = 128) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    np_batch: (N,H,W[,C]) float32 in [0,1] or uint8 0..255\n",
    "    Returns: labels in original space {-1,0,1}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if np_batch.ndim == 3:\n",
    "        np_batch = np.expand_dims(np_batch, axis=0)\n",
    "\n",
    "    # (N,H,W,C) -> (N,C,H,W) if needed\n",
    "    if np_batch.shape[-1] in (1, 3) and np_batch.shape[1] not in (1, 3):\n",
    "        np_batch = np.transpose(np_batch, (0, 3, 1, 2))\n",
    "    if np_batch.dtype != np.float32:\n",
    "        np_batch = np_batch.astype(\"float32\")\n",
    "    if np_batch.max() > 1.5:\n",
    "        np_batch /= 255.0\n",
    "\n",
    "    # Channel alignment\n",
    "    if np_batch.shape[1] != in_channels:\n",
    "        if in_channels == 1 and np_batch.shape[1] == 3:\n",
    "            np_batch = np_batch.mean(axis=1, keepdims=True)  # RGB->gray\n",
    "        elif in_channels == 3 and np_batch.shape[1] == 1:\n",
    "            np_batch = np.repeat(np_batch, 3, axis=1)        # gray->RGB\n",
    "        else:\n",
    "            raise ValueError(f\"Channel mismatch: expected {in_channels}, got {np_batch.shape[1]}\")\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, np_batch.shape[0], batch_size):\n",
    "            chunk = torch.from_numpy(np_batch[i:i+batch_size]).to(device, non_blocking=True)\n",
    "            # Resize to 224 to match training behavior\n",
    "            _, C, H, W = chunk.shape\n",
    "            if H < 224 or W < 224:\n",
    "                chunk = F.interpolate(chunk, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "            logits = model(chunk)\n",
    "            preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    return np.vectorize(index_to_label.__getitem__)(preds)\n",
    "\n",
    "# Example:\n",
    "# sample_preds = predict_labels_numpy(X_val[:8].cpu().numpy())\n",
    "# print(\"Sample preds:\", sample_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
