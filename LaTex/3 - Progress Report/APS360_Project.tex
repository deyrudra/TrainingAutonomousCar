\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Progress Report}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{4}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption} 
\restylefloat{table}

%######## APS360: Put your project Title here
\title{Progress Report: AUTONOMOUS CAR USING CNN\\}


%######## APS360: Put your names, student IDs and Emails here
\author{Rudra Dey  \\
Student\# 1010124866\\
\texttt{rudra.dey@mail.utoronto.ca } \\
\And
Pravin Kalaivannan  \\
Student\# 1010141295 \\ 
\texttt{pravin.kalaivannan@mail.utoronto.ca} \\
\AND
Aadavan Vasudevan  \\
Student\# 1010101514 \\
\texttt{aadavan.vasudevan@mail.utoronto.ca} \\
\And
Abishan Baheerathan \\
Student\# 1010218756 \\
\texttt{abishan.baheerathan@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}

\maketitle


\section{Project Description}

This project involves training an simulated autonomous car using a convolutional neural network (CNN) and a forward-facing camera. The goal of this project is to build a model that can predict steering commands based on visual input from the camera. This should allow the car to drive on its own without the need of additional sensors or manual commands.  

Our team’s motivation behind the project was to explore a simplified, small-scale version of autonomous driving systems, using deep learning. Furthermore, traditional rule-based approaches are limited by their inability to predict the vast number of driving scenarios that can occur. Deep learning offers the ability to learn from data and generalize across different visual conditions. This made it a suitable choice for handling the complex input from a moving car. The project also provided insight into the effectiveness of vision-only autonomous systems and the role of deep learning in embedded robotics.


\section{Individual Contributions and Responsibilities}

Our team worked collaboratively over the duration of the project with clear task divisions and weekly milestones. All communication was handled through Discord, where we maintained an active group chat with frequent updates and responses. We held two scheduled meetings every week, and used GitHub to track code progress, manage issues, and handle version control across all components. The team has a shared Google Drive where documents are added for collaboration. 


\subsection{Individual Contributions}
Each team member was assigned specific responsibilities:

\begin{itemize}
  \item \textbf{Aadavan} handled the baseline rule-based model used for comparison, authored the ethical considerations section, and participated in model evaluation.

  \item \textbf{Rudra} led the software setup and camera mounting, collected the training dataset, and worked on preprocessing the image data.

  \item \textbf{Pravin} contributed to background research on similar projects and built the CNN architecture. Helped create the dataset in CARLA by collecting images and steering values of cars.

  \item \textbf{Abishan} supported data preprocessing, designed the training setup (optimizer, loss function, hyperparameters), created the project's risk register, and contributed to model training.
\end{itemize}

\subsection{Timeline of Completed Tasks}

We maintained an internal task schedule which helped the team stay on track. The following table summarizes the tasks completed by each team member, their completion dates, and the outcomes of each task.

\begin{table}[h]
\centering
\caption{Individual Contributions and Timeline}
\vspace{0.5em}
\begin{tabular}{|p{4.5cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\textbf{Task} & \textbf{Team Members} & \textbf{Completion Date} & \textbf{Outcome} \\ \hline
Brainstorm Project Ideas & All Members & June 9, 2025 & \\ \hline
Model Selection & All Members & June 10, 2025 & \\ \hline
Project Proposal (Draft \& Final) & All Members & June 12–13, 2025 & \\ \hline
Data Collection \& Setup & Rudra, Aadavan, Pravin & June 20, 2025 & \\ \hline
Data Preprocessing & Rudra, Abishan & June 27, 2025 & \\ \hline
Baseline Model Implementation & Aadavan & June 30, 2025 & \\ \hline
Architecture \& Model Building & Pravin, Aadavan & July 4, 2025 & \\ \hline
Training Regime Development & Abishan, Pravin & July 8, 2025 & \\ \hline
Model Training \& Tuning & Abishan, Rudra & July 10, 2025 & \\ \hline
Progress Report & All Members & July 11, 2025 & \\ \hline
\end{tabular}
\end{table}


\section{Data Processing}

Data was collected from the CARLA simulator, including images and corresponding steering values. We then normalized the images allowing us to train our CNN.
Further, data augmentation was applied to increase the size and diversity of the dataset, leading to better generalization of the model.


\subsection{Data Collection and Cleaning}

In CARLA, there is vehicles that have autonomous functionality, which we used to collect data.
We attached an RGB Camera Sensor to the car to record 10 frames per second (FPS) as the car
travels around a designated map. In addition to the images, data for the steering values and turn signals of the car
was collected allowing for use as ground truth labels for training the model. This data collection was done on diffreent maps with different weather conditions
to ensure a diverse dataset. We collected samples on three maps, in which the car collected data for 30 minitues, resulting in 54000 total images.


To clean the data, we removed any images that were all black, duplicates or not the correct resolution of 320x240 leaving us with 53457 images.  We then saved 
the data as numpy arrays, zipped and uploaded to a shared Google Colab file along, ensuring access for all members. 

Procedure:
\begin{enumerate}
  \item{Spawn in a vehicle}
  \item{Attach an RGB Camera Sensor (front-facing) to the dash of the car.}
  \item{Collect image data (10 fps) along with steering data and turn signals.}
  \item {Clean the data by removing corrupted images and duplicates.}
  \item {Save the data as numpy arrays for easy access and processing.}
\end{enumerate}


\subsection{Data Preprocessing}
All collected images were resized to a resolution of 160x120 pixels, with the pixel values normalized to a range of [0, 1]. 
This was done to ensure consistency in the input data, allowing the model to learn effectively from the images.

\begin{figure}[H] % h = here, t = top, b = bottom, p = page of floats
    \centering
    \includegraphics[width=0.6\textwidth]{exampleofdataset.png} % Adjust path and size
    \caption{Resized 3x160x120 example images from dataset}
    \label{fig:example}
\end{figure}



\subsection{Data Distribution}
We decided to do a 80/10/10 split for the training, validation and test sets respecivelty. This was done to ensure that have a
large set of data for training, while also having enough data for validation and testing. Table 3 shows the distribution of the data.

\begin{table}[h]
\centering
\caption{Dataset Distribution}
\vspace{0.5em}
\begin{tabular}{|p{2cm}|p{2cm}|}
\hline
\textbf{Dataset Split} & \textbf{Total Images} \\ \hline
Training Set           & 42766 \\ \hline
Validation Set         & 5345 \\ \hline
Test Set               & 5346 \\ \hline
Total Images           & 53457  \\ \hline
\end{tabular}
\end{table}


\subsection{Data Agumentation}

Data augmentation was applied to the training set to increase the size and diversity of the dataset. We applied two tenchniquies to generalzie the model and forces
it to learn critical features. 

\begin{itemize}
  \item \textbf{Greyscale Conversion:} All training images were converted to greyscale reducing model's reliance on color-based features, encouraging learning of more genreal visual features/road patterns. This will allow the model to improve, and work in different lighting conditions or if the camera is distorted.

  \item \textbf{Cutout Augmentation:} A rectangular region of each training image was masked out to simulate real-world situations (e.g. raindrop or debris on the camera lens). This encoruses the model to learn relevant features from different parts of the image making it more robust overall. 
\end{itemize}

These agumentations lead to the training set being increased to 128298 images, with each image having two copies with each augmentations applied respecivelty.

\begin{figure}[H] % h = here, t = top, b = bottom, p = page of floats
    \centering
    \includegraphics[width=0.6\textwidth]{exampleofaugmented.png} % Adjust path and size
    \caption{Augmented image examples (greyscale and cutout)}
    \label{fig:example}
\end{figure}


\begin{table}[h]
\centering
\caption{Dataset Distribution after Training Set Augmentation}
\vspace{0.5em}
\begin{tabular}{|p{2cm}|p{2cm}|}
\hline
\textbf{Dataset Split} & \textbf{Total Images} \\ \hline
Training Set           & 42766 x 3 =  128298\\ \hline
Validation Set         & 5345 \\ \hline
Test Set               & 5346 \\ \hline
Total Images           & 138989 \\ \hline
\end{tabular}
\end{table}


\subsection{Plan for Testing on Unseen Data}

In order to evaluate the model's performance, we will be testing it on unseen data which we reserved 10\% of the dataset for. Given how 
the data was collected sequentially across different maps and weather conditions, the test set will inheritely contain frames the model has not encountered before.
This will provide the model differnt roads and turns to encounter and we can see how well it generalizes to new situations.

Additionally, to further test the model's generalization, we can load the model in CARLA and make it drive around a new map where data was not collected from, and see how well the model performs in 
real-time. This will allow us to visualize the model's performance and see how well it can adapt to new environments.

\subsection{Challeneges Faced and Solutions}

During the data collection process, we faced several challenges and overcame them with the following solutions:

\begin{itemize}
  \item \textbf{Slow Initial Data Collection:} The simulator originally ran in real time, meaning that collecting data for 30 minitues would take that long in real time. 
  To solve this issue, we sped up the simulation, allowing us to collect data at a faster rate and collect more data in a shorter time frame.

  \item \textbf{Turn Signal Data Collection:} Collecting turn signal data was initially problematic, 
  because the vehicle's indicators were not being used by the automated vehicle in CARLA. 
  We tried to resolve this through manual activation based on steering angle, but this proved to be innacurate and inconsistent. 
  We resolved this by enabling the CARLA Traffic Manager’s automatic lights, allowing turn signals to activate based on the vehicle's navigation. \
  In turn, allowing us to have accurate turn signal data for the training set. 

\end{itemize}
 
\section{Baseline Model}

Using a Ridge Regression Algoirthm we created a baseline model that predicts steering angles for a self-driving car 
from grayscale camera images and turn signal inputs. This baseline model serves as a simple, interpretable baseline
such that we can compare against our more complex primary neural network model later. To collect data, we used a simulator
called CARLA, to obtain camera images, turn signal inputs, and our ground truth label the steering angles.


\subsection{Ridge Regression with Image Features}

Initially, the grayscale images (of shape 160x120) were flattened into 1D feature vectors and normalized. The features themselves
represent the visual input of the car's front-facing camera. Additionally, left and right turning signals were captured as an
additional feature; combining this with our flattened grayscale images we were left with a numpy array of shape $N \times 19201$,
where $N$ represents the number of images, and 19201 are the number of feature column vectors. The Ridge Regression algorithm was
selected because it penalized large coefficients ($L_2$ norm), allowing for a generalized model. A series of models were trained
with different regularization strengths ($\alpha$), and the performance was evaluated using Mean Squared Error (MSE) and $R^2$ Score
on a held-out 20\% test set.

\subsection{Minimal Tuning of $\alpha$}
The only hyperparameter needing to be tuned was the regularization strength for the Ridge Regression Algorithm. This was manually tuned,
and thus no validation set was used. A small set of candidate values for $\alpha$ was chosen: $\alpha \text{ } \epsilon \text{ } 
\{ 1, 10, 100, 250, 1000, 8000\}$ With each value of $\alpha$, the model was trained and tested with a dataset of 15 minutes of simulated
driving. By obtaining the average MSE and the average $R^2$ score, our fourth model of $\alpha = 100$ came out to be the best. 


\begin{figure}[H] % h = here, t = top, b = bottom, p = page of floats
    \centering
    \includegraphics[width=0.6\textwidth]{model4learningcurve.png} % Adjust path and size
    \caption{This is the learning curve for "Model 4" a Ridge Regression Model with $\alpha = 100$}
    \label{fig:model4learningcurve}
\end{figure}

\subsection{Overfitting and Model Quality}
As shown in Figure~\ref{fig:model4learningcurve}, the results are suggesting some overfitting due to the models tendency
to perform much better on training data than unseen data. This is reasonable since the training data frames, 
are all consecutive in nature, and the dataset used isn't extremely large either. In addition, as the training size
increase the gap between Test and Train MSEs narrow, suggesting that with more data the model will generalize better. 
The final test MSE is (~0.0025) which is quite low, meaning the model performs well overall after training on the data.


\subsection{Qualitative Observations}
When running the steering angle prediction model in CARLA simulator, the model exhibited promising behaviour in terms of
autonomous driving capabilities, however with large limitations. The car was able to drive freely, using the model's predictions
from the front-facing grayscale camera images and turn signals provided by the user. In Figure~\ref{fig:model4demo} we can see 
the model taking a left turn in an intersection.

\begin{itemize}
  \item \textbf{Turn Signal Response:} The car consistently turned in the correct direction when prompted, showing that the model incorporated turn signal input effectively.

  \item \textbf{Obstacle Avoidance:} The vehicle generally avoided obstacles but occasionally clipped walls, suggesting basic spatial awareness but limited precision.

  \item \textbf{Lane-Keeping:} The car had trouble staying within lane boundaries, often drifting, especially during turns or on complex roads.

  \item \textbf{Map Generalization:} Despite limitations, the car performed reasonably well across different map layouts, showing decent control over turns.
\end{itemize}


\begin{figure}[H] % h = here, t = top, b = bottom, p = page of floats
    \centering
    \includegraphics[width=1.0\textwidth]{model4demo.png} % Adjust path and size
    \caption{This is the demo for "Model 4" in CARLA}
    \label{fig:model4demo}
\end{figure}


\section{Training Neural Netowrk}

*Quantitative Results: Training Loss (MSE) and Validation Loss

\subsection{Model Training Script}

\subsection{Logging}

\subsection{Outputting Results}



\section{Evaluating with Inference}

*Qualitative Results: Using model for actual driving in CARLA

\subsection{Running model in CARLA using live camera feed}


\end{document}
