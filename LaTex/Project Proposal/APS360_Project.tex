\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
%\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{4}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\restylefloat{table}

%######## APS360: Put your project Title here
\title{Autonomous Car using Deep Learning Project Proposal\\}


%######## APS360: Put your names, student IDs and Emails here
\author{Rudra Dey  \\
Student\# 1010124866\\
\texttt{rudra.dey@mail.utoronto.ca } \\
\And
Pravin Kalaivannan  \\
Student\# 1010141295 \\
\texttt{pravin.kalaivannan@mail.utoronto.ca} \\
\AND
Aadavan Vasudevan  \\
Student\# 1010101514 \\
\texttt{aadavan.vasudevan@mail.utoronto.ca} \\
\And
Abishan Baheerathan \\
Student\# 1010218756 \\
\texttt{abishan.baheerathan@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}

\maketitle

\begin{abstract}
This document presents our team's approach to developing an autonomous car system. This document contains research done on similar projects, data processing methods, 
architecture details, and a baseline comparison model. Our team looked into ethical 
concerns and potential major risks that could occur with this project. Further, the team came 
together to form a detailed project plan. This plan contains important tasks, 
who is responsible for each task, and project deadlines.\\
%######## APS360: Do not change the next line. This shows your Main body page count.
Total Pages: \pageref{last_page}
\end{abstract}



\section{Introduction}


This project proposal will discuss our team’s plan to train an 
autonomous car. The motivation behind this project comes from a shared interest in robotics and deep 
learning among the team. Our team’s goal is to train the car to drive itself using cameras and control 
input. This project will be using a CNN model, public datasets such as the Udacity self-driving car dataset, 
and custom data that is collected through a mounted camera on an RC car. This is an interesting project 
because we get to explore the world of autonomous vehicles and the only sensor we plan on using is a camera. 
This project needs deep learning to process data in a fast and reliable way. We need to code procedural 
instructions for different situations based on our sensor (the camera), but this seems dangerous as humans 
can not think of every possible situation. A training model can handle this problem in a generalized way 
which is what we need to accomplish our goal. 
 



\section{Background and Related Works}
\label{headings}

\subsection{Autonomous Car Related Projects}

Through the use of deep learning, many autonomous car projects have been developed and tested, showing the potential of this technology in real-world applications. 
The following is an overview of five works related to autonomous vehicles using deep learning.  


\subsubsection{Deep CNN End-to-End Learning for Autonomous RC Cars \citep{bhutta2023deep}}

 
In this study, end-to-end learning using deep convolutional neural networks (CNNs) was used to autonomously control an RC car around a race track. 
A front-facing camera is used to collect images of the car’s view, which is fed to the model. The CNN extracts features through convolutional and pooling layers, 
then applies the nonlinear ReLU activation function, which is then passed through 3 fully connected layers to generate the output of necessary steering angles 
and speed the car needs to travel \citep{bhutta2023deep}.

\subsubsection{Autonomous RC Car Using Neural Networks \citep{Mallik2023}}

This project explores autonomous driving through a small RC car using a Raspberry Pi and a CNN based end-to-end steering angle prediction system. 
To train the model, the car was manually driven around a track and the Raspberry Pi was used to track the analog steering inputs along with recording video frames 
taken at 10-20 fps. The project highlights limited computing resources from the Pi’s onboard chips which may constrain CNN complexity along with data 
diversity being an issue where overfitting to a single environment leads to issues in other environments. Additionally, the project highlights the 
feasibility of deep learning for autonomous navigation and that it can be scaled up to larger robotic systems.

\subsubsection{Development of Single-board Computer-based Self-Driving Car Model \citep{9751873}}

In this paper, an RC car based on the DonkeyCar project, powered by a Nvidia Jetson Nano and controlled by a CNN was used to drive autonomously around a track. The
CNN uses live photos of the car’s front view to predict the steering angle and speed required. The authors created varied training set sizes of 
3000, 6000, 12000 and 24000 along with variations of network depth of between two and five layers of convolutional layers to find the best model \citep{9751873}. 
Their results showed that the model with 24000 images with three convolution layers performed the best with an absolute error at 0.18257 \citep{9751873}. 
Additionally, the results showed that having more data consistently reduced error, while adding layers above three had diminishing returns. 


\subsubsection{CNN based End to End Learning Steering Angle Prediction for Autonomous Electric Vehicles \citep{Mygapula2021}}

This paper explores CNN based end-to-end learning for steering angle prediction in autonomous vehicles using images captured of the front 
view of the car which is passed through to the model in a Jetson TX1. The team trained 3 different models based on different CNN architectures 
with different numbers of layers and the results showed that the CNN model with 4 CNN layers and 4 connected layers performed the 
best with 0.0354 test loss \citep{Mygapula2021}. These results highlight the viability of CNNs in learning steering behaviours compared to typical 
approaches in which systems are broken into multiple stages (such as road and object detection, and path planning)as it has fewer potential failure points.

\subsubsection{End to End Learning for Self-Driving Cars \citep{bojarski2016endendlearningselfdriving}}

In this paper, the team trained a CNN to map raw pixels from a front-facing camera directly to steering commands for a car. 
The network made up of nine layers, with one normalization layer, five convolutional layers and three fully connected layers 
was trained to minimize the mean squared error between steering commands and the human driver’s inputs. Using only 
72 hours of driving data, the team was able to successfully train the car to operate in diverse conditions showing 
how a CNN can perform the entire task without manual decomposition into smaller systems \citep{bojarski2016endendlearningselfdriving}. 

\section{Data Processing}
\label{headings}


The Data Processing pipeline for the self-driving car project has three critical phases:
Data Collection, Data Cleaning and Preprocessing, and Dataset Splitting. These phases are
needed to ensure proper and high-quality training, validation, and testing of our model.

\subsection{Data Collection}

\subsubsection{Public Datasets}
\begin{itemize}
  \item Udacity Self-Driving Car Dataset \citep{udacity_self_driving_car_2021}
  \item Berkeley DeepDrive (BDD100K) \citep{bdd100k_2018}
\end{itemize}

\subsubsection{Custom Data Collection}
\begin{itemize}
  \item Simulated Data: Using CARLA Simulator to generate high-resolution images with corresponding control commands \citep{Dosovitskiy17}
  \item Real-World Data: Dash-mounted Camera on RC Cars.
\end{itemize}


\subsection{Data Cleaning and Preprocessing}

This phase involves preparing the image data and labels to maximize model accuracy. 
This includes image normalization and label consistency checks.

\subsubsection{Remove Corrupted or Blurry Frames}
Removal of incomplete metadata, heavy blur, overexposure, and distorted images using automated filters.
These filters include: Laplacian Variance for blur detection, Histogram Analysis for overexposure, 
and Pixel Clipping Detection.


\subsubsection{Image Resizing and Normalization}
All images are resized to a uniform size of 320x240 pixels to match input expectations for deep CNNs. Additionally, 
pixel values are normalized to seed up convergence during training.

\subsubsection{Label Alignment and Verification}
Control labels are synchronized with their corresponding images using timestamps. Outliers and inconsistent 
data are flagged and removed. 

\subsection{Dataset Splitting}
The dataset is split into:
\begin{itemize}
  \item 80\% Training
  \item 10\% Validation 
  \item 10\% Testing 
\end{itemize}

Since we are using data that comes from continuous streams, randomly shuffling data could have
it so very similar frames of the stream can land in training, validation, and testing leading to 
memorization of the data. Thus, we will split the data chronologically, first 80\% goes to 
training, the next 10\% goes to validation, and the last 10\% goes to testing. This way the test set 
includes unseen time segments from the drive. 


\section{Architecture}

The neural network architecture is designed to take images of size 320x240x3 (height, width, RGB) of the
front-facing camera on the dash or hood of the car. With this, the neural network outputs/predictions are the 
continuous steering angle values for the car (regression task). Using multiple Conv2d layers, with 5x5 kernels, 
a stride of 1, and a ReLU activation to introduce non-linearity we aim to extract spatial features, like 
lane lines, edges, and road curvature. These feature maps are passed through a flattened layer and then into 
a fully connected layer which allows the model to learn high-level representations. The final output layer is a single neuron
with a linear activation function capturing the steering angle of the car. This implies that the speed of the car
is constant from the start, or could be controlled manually.

\subsection{Normalization}
To improve generalization we:
\begin{itemize}
  \item Batch normalization is applied after convolutional layers to normalize activations.
  \item L2 Weight Decay is applied during optimization to penalize overly complex models which leads to overfitting.
\end{itemize}

\subsection{Training Method}
This network is trained as a regression problem, where the goal is to minimize the error between predicted and actual
steering angles. We will use the following to train our model:
\begin{itemize}
  \item Loss Function: Using Mean Squared Error (MSE) to calculate the loss, penalizing large changes in steering
  predictions.
  \item Optimizer: Using the Adam Optimizer we will update network eights during training with adaptivative learning rates.
  \item Training Metrics: Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are used to evaluate the accuracy of steering predictions.
\end{itemize}



\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the format being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and years (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

To cite a new paper, first, you need to add that paper's BibTeX information to \verb+APS360_ref.bib+ file and then you can use the \verb|\citep{}| command to cite that in your main document. 

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figs/td-deep-learning.jpg}
\end{center}
\caption{Sample figure caption. Image: ZDNet}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}



\section{Baseline Model}

A hand-coded rule-based controller serves as a reasonable baseline for evaluating the performance of the autonomous car. 
This baseline model simulates traditional line-following logic, where decisions are made using simple heuristics based on 
pixel colour thresholds and pre-defined turning logic \citep{LIKMETA2020103568}. The rule-based controller follows a deterministic flow, first, the incoming 
video frame is converted to grayscale or HSV colour space, and then a threshold is applied to detect the track line (usually a black line 
on a white surface or vice versa) \citep{LIKMETA2020103568}. The centroid of the detected line is computed, and based on its position relative to the image center, 
turning decisions are made. For example, steering left if the line is on the left half of the image, right if on the right half, and forward if 
centered \citep{bojarski2016endendlearningselfdriving}. This model does not involve learning or generalization; it is purely reactive and works well in constrained, consistent environments 
with high-contrast tracks. While simplistic, this approach is widely used as a baseline in autonomous driving projects due to its reproducibility 
and interpretability.

\section{Ethical Considerations}

\subsection{Model Usage}
Since this project involves physical hardware operating in real time, safety is a primary concern. The model may make poor decisions in edge cases 
(obstacles, unfamiliar lighting conditions), which could lead to hardware damage or unintended collisions \citep{dalrymple2024guaranteedsafeaiframework}. If used as part of a demonstration 
involving human interaction, ensuring safety measures (kill switches, limited speed) is essential. Another ethical concern is over-reliance on the 
model's predictions. If used in future deployments (real vehicles or educational kits), assuming the trained model can handle all situations without 
proper validation may mislead users and cause harm or accidents \citep{laskey2017comparinghumancentricrobotcentricsampling}. 


\subsection{Data Collection}
The data used for imitation learning is sourced from human demonstrations. A bias may arise if only a single driving style or track layout is captured.
For instance, if the human driver consistently takes tight turns or drives aggressively, the model may learn to imitate that behaviour, limiting 
generalization to new tracks or drivers. Additionally, reinforcement learning episodes are generated in a simulated or controlled environment 
\citep{laskey2017comparinghumancentricrobotcentricsampling}. 
This might restrict the model's robustness in diverse real-world settings. Ethical considerations also include ensuring no unnecessary wear is inflicted 
on hardware during data collection or that any modifications to the car setup are clearly documented and standardized.

\section{Project Plan}
To ensure the seamless progression of the project and collaboration among team members, we have devised guidelines and a timeline as seen below.

\subsection{Team Guidelines}

\begin{table}[H]
\caption{Meeting Specifications}
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Meeting Guidelines} & \textbf{Details} \\ \hline
In-person meeting           & Location and time decided by the team \\ \hline
Online meetings             & Tuesdays (3-5 pm) and Saturdays (1-3 pm) \\ \hline
Absence Notification        & Must notify team 24 hours before the meeting if they will be absent \\ \hline
\end{tabular}
\end{center}
\end{table}

\textbf{Communication Guidelines}
\begin{itemize}
  \item Group chat is active on weekdays and weekends (24/7)
  \item Group chat is active on weekdays and weekends (24/7)
  \item Members are required to frequently check discord messages (less than 2 hour response time)
\end{itemize}

\textbf{Collaboration Guidelines}
\begin{itemize}
  \item The team will be working together in a GitHub repository
  \item Each member is required to give a daily update message on their progress
  \item If conflicts arise, the team must discuss them together and decide on a solution that makes everyone happy
  \item Ensure the code has informational comments that help the team understand it
\end{itemize}

\subsection{Project Timeline}



\section{Risk Register}
The risk register contains some scenarios that could negatively impact our project. 
These potential risks negatively affect deadlines, quality of work, and more. Our team has 
discussed these scenarios and come up with solutions for each project risk. 

\begin{table}[H]
\caption{Project Risks}
\centering
\begin{tabular}{|p{4cm}|p{2.5cm}|p{7cm}|}
\hline
\textbf{Project Risk} & \textbf{Likelihood} & \textbf{Solution} \\ \hline
A teammate drops the course & Unlikely & The team has to approach splitting up tasks differently as teammates will now have more responsibilities. We have to start tasks earlier and move internal deadlines to an earlier date because each member has more tasks to complete. \\ \hline
Model training takes longer than expected & Likely & It’s common to procrastinate on less important or easier tasks, so this would be the time to complete them. The team should start working on tasks earlier, so if this does occur it doesn’t affect project deadlines. \\ \hline
Experience hardware problems during testing & Likely & The team should keep extra hardware components in case they fail the during testing stages. Also, try not to overuse the physical components, and use simulation software to test. \\ \hline
The model works in simulation but not on a physical RC car & Likely & We shouldn’t fully rely on simulations and should test the model on the physical RC car frequently. The earlier the team faces these issues, the longer we have to fix them. Compare the results from simulation and physical RC car and that might help solve problems. \\ \hline
If teammate misses an internal deadline & Likely & The team should hold everyone accountable as each member is responsible for their assigned tasks. The teammate should finish tasks as soon as possible after the missed deadline because this will hold back the team. \\ \hline
\end{tabular}
\end{table}


\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textsc{References} section; see below). Please note that pages should be
numbered.


\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
