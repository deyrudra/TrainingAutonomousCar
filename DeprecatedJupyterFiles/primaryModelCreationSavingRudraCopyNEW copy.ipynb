{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8cf26599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Dataset ==================\n",
    "class CarlaDataset(Dataset):\n",
    "    def __init__(self, images, angles, signals, transform=None):\n",
    "        self.images = images\n",
    "        self.angles = angles\n",
    "        self.signals = signals\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        angle = self.angles[idx]\n",
    "        signal = self.signals[idx]\n",
    "\n",
    "        # only for straight cases:\n",
    "        if signal == 0.0:\n",
    "            if abs(angle) <= 0.05:\n",
    "                angle = 0.0\n",
    "            elif angle > 0.05:\n",
    "                angle = angle * 0.45\n",
    "            # if angle < -0.05, leave it unchanged\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(img, dtype=torch.float32),\n",
    "            torch.tensor(signal).unsqueeze(0),\n",
    "            torch.tensor(angle, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9bf51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model ==================\n",
    "class SteeringModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SteeringModel, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "\n",
    "        self.signal_fc = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_left = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.fc_right = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.fc_straight = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, signal):\n",
    "        x = self.cnn_backbone(image)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        s = self.signal_fc(signal)\n",
    "        combined = torch.cat([x, s], dim=1)\n",
    "\n",
    "        preds = torch.zeros(image.size(0), 1, device=image.device)\n",
    "        left_mask = (signal.squeeze(1) == -1)\n",
    "        right_mask = (signal.squeeze(1) == 1)\n",
    "        straight_mask = (signal.squeeze(1) == 0)\n",
    "\n",
    "        if left_mask.any():\n",
    "            preds[left_mask] = self.fc_left(combined[left_mask])\n",
    "        if right_mask.any():\n",
    "            preds[right_mask] = self.fc_right(combined[right_mask])\n",
    "        if straight_mask.any():\n",
    "            preds[straight_mask] = self.fc_straight(combined[straight_mask])\n",
    "\n",
    "        return preds.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "278ddb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Loss ==================\n",
    "class DirectionalWeightedMSE(nn.Module):\n",
    "    def __init__(self, left_weight=0.0, right_weight=0.0, straight_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.left_weight = left_weight\n",
    "        self.right_weight = right_weight\n",
    "        self.straight_weight = straight_weight\n",
    "\n",
    "    def forward(self, preds, targets, signals):\n",
    "        weights = torch.ones_like(targets)\n",
    "        weights[signals.squeeze() == -1] = self.left_weight\n",
    "        weights[signals.squeeze() == 1] = self.right_weight\n",
    "        weights[signals.squeeze() == 0] = self.straight_weight\n",
    "        loss = weights * (preds - targets) ** 2\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f958829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Load Data ==================\n",
    "images = np.load('../output/std_images.npy')         # (N, 3, 224, 224)\n",
    "angles = np.load('../output/std_angles.npy').astype(np.float32)\n",
    "signals = np.load('../output/std_turn_signals.npy').astype(np.float32)\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(images)),\n",
    "    test_size=0.2,\n",
    "    shuffle=False  # No shuffling for time series data\n",
    ")\n",
    "\n",
    "train_data = (images[train_idx], angles[train_idx], signals[train_idx])\n",
    "val_data   = (images[val_idx],   angles[val_idx],   signals[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87550ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Transforms ==================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5355b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "    CarlaDataset(*train_data, transform=transform),\n",
    "    batch_size=32, shuffle=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    CarlaDataset(*val_data, transform=transform),\n",
    "    batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83f05aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pravin\\AppData\\Local\\Temp\\ipykernel_11564\\3279276879.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(img, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train 0.0092 | Val 0.0059\n",
      "  → Checkpoint saved (val 0.0059)\n",
      "Epoch 2 | Train 0.0035 | Val 0.0036\n",
      "  → Checkpoint saved (val 0.0036)\n",
      "Epoch 3 | Train 0.0029 | Val 0.0047\n",
      "Epoch 4 | Train 0.0027 | Val 0.0178\n",
      "Epoch 5 | Train 0.0028 | Val 0.0076\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# — assume CarlaDataset and SteeringModel are defined above —\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = SteeringModel().to(device)\n",
    "\n",
    "# where to save / load\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "checkpoint_path = \"../models/RDModel_checkpoint.pth\"\n",
    "\n",
    "# — load checkpoint if it exists —\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    start_epoch   = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt['best_val']\n",
    "    print(f\"Resuming from epoch {ckpt['epoch']} (best_val={best_val_loss:.4f})\")\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch\")\n",
    "    start_epoch   = 1\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "# — define per-component learning rates —\n",
    "lr_backbone = 1e-3    # fine-tune ResNet backbone & signal encoder\n",
    "lr_left     = 1e-3    # left-turn head\n",
    "lr_right    = 1e-3    # right-turn head\n",
    "lr_straight = 1e-3    # straight-ahead head\n",
    "\n",
    "# — build optimizer with separate param groups —\n",
    "optimizer = torch.optim.Adam([\n",
    "    { \"params\": model.cnn_backbone.parameters(), \"lr\": lr_backbone },\n",
    "    { \"params\": model.signal_fc.parameters(),    \"lr\": lr_backbone },\n",
    "    { \"params\": model.fc_left.parameters(),      \"lr\": lr_left    },\n",
    "    { \"params\": model.fc_right.parameters(),     \"lr\": lr_right   },\n",
    "    { \"params\": model.fc_straight.parameters(),  \"lr\": lr_straight},\n",
    "], weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# — prepare data loaders —\n",
    "images  = np.load('../output/all_images.npy')\n",
    "angles  = np.load('../output/all_angles.npy').astype(np.float32)\n",
    "signals = np.load('../output/all_turn_signals.npy').astype(np.float32)\n",
    "\n",
    "idxs      = np.arange(len(images))\n",
    "train_idx, val_idx = train_test_split(idxs, test_size=0.2, shuffle=False)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    CarlaDataset(images[train_idx], angles[train_idx], signals[train_idx], transform),\n",
    "    batch_size=32, shuffle=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    CarlaDataset(images[val_idx],   angles[val_idx],   signals[val_idx],   transform),\n",
    "    batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "# — training loop —\n",
    "num_epochs = 5\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train = 0.0\n",
    "    for imgs, sigs, angs in train_loader:\n",
    "        imgs, sigs, angs = imgs.to(device), sigs.to(device), angs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs, sigs)\n",
    "        loss  = criterion(preds, angs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train += loss.item()\n",
    "    train_loss = total_train / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, sigs, angs in val_loader:\n",
    "            imgs, sigs, angs = imgs.to(device), sigs.to(device), angs.to(device)\n",
    "            total_val += criterion(model(imgs, sigs), angs).item()\n",
    "    val_loss = total_val / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train {train_loss:.4f} | Val {val_loss:.4f}\")\n",
    "\n",
    "    # Checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch':       epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'opt_state':   optimizer.state_dict(),\n",
    "            'best_val':    best_val_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  → Checkpoint saved (val {best_val_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c2b8386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned weights to ../models/RDModel_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"../models/RDModel_finetuned.pth\"\n",
    ")\n",
    "print(\"Saved fine-tuned weights to ../models/RDModel_finetuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
